# Roadmap

## v0.1.0

### Release Deliverables

- Finalize BlockStore and BlobStore APIs.
- Provide filesystem and S3 implementations with configuration docs.
- Ship CLI for ingesting and fetching files.
- Expose HTTP gateway for remote access.
- Publish basic metrics and structured logging.
- Deliver getting-started and backend configuration guides.
- Ensure integration tests cover core storage paths.
- Set up CI workflows and artifact publishing.

### Core Ingest Pipeline

Build an anchored, format-aware ingest flow that processes each file in a
single streaming pass:

1. **Transport decode** – strip content encodings such as gzip before
   analysis.
2. **Format sniffing** – inspect the first 8–16 KiB to detect the container
   (PDF, ZIP, JPEG, text, etc.) and select the corresponding token pack.
3. **Anchored tokenization** – inject format-aware markers (for example PDF
   `stream`/`endstream`, ZIP headers, JPEG SOI/EOI, or newlines for text).
4. **Content-defined chunking within anchors** – run FastCDC or Gear inside
   each anchored span to pick stable boundaries.
5. **Chunk handling** – for every plaintext chunk compute a
   content-addressable hash, decide whether to compress, encrypt using AEAD
   with per-chunk nonces, and emit a self-describing frame that records the
   metadata required for replays.
6. **Manifest generation** – maintain an ordered manifest of
   `{hash_plain, size_plain, offsets}` referencing the stored chunks so that
   random access remains trivial.

### Tokenizer and CDC Implementation

- Implement a compact DFA/Aho-Corasick matcher to identify token packs over a
  sliding window.
- Provide a `ZPipeline.anchoredCdc(tokenPack, avgSize, anchorBonus)` built on
  `ZSink.foldWeightedDecompose`, combining byte-growth cost with anchor
  bonuses.
- Introduce a fixed 1 MiB staging rechunker to cap memory usage before
  handing data to the CDC pipeline.

### Self-Describing Frame Format

- Define a little-endian header containing the `"QUASAR"` magic, version,
  flags, algorithm identifiers, sizes, nonce, truncated plaintext hash,
  key identifier, and optional fields (file ID, chunk index, dictionary ID).
- Canonically encode these fields as Additional Authenticated Data so that
  decrypt/verify operations can be plugged in without bespoke wiring.
- Ensure each successful decode yields a single `Take.chunk` and that
  failures surface through `Take.fail` without double terminals.

### Deduplication Strategy

- Store only the base blocks generated by anchored CDC and use manifests to
  rebuild files.
- Support future CDC configuration changes by updating manifests while
  reusing existing blocks.
- Plan a rolling-hash index for cross-file containment detection without
  slicing below the base block size.

### Format-Aware Views

- Preserve the canonical file bytes while generating asynchronous views that
  produce format-specific fingerprints and seek maps.
- Provide a `PDFView` capable of mapping objects/pages and hashing lossless
  stream payloads for deduplication/search.
- Extend `ZIPView` to surface central directory entries and enable lazy
  retrieval through manifest offsets.

### Operational Guardrails

- Apply backpressure with bounded `Queue[Take]` instances.
- Close wrapped `OutputStream`s on both success (`Take.end`) and failure
  (`Take.fail` only).
- Cap decode efforts during sniffing and format parsing to guard against
  resource exhaustion.
- Enforce `./sbt scalafmtAll` pre-commit checks to maintain formatting.

## Future

- Additional blob store backends.
- Advanced caching and deduplication strategies.
- High-level SDKs for common languages.
